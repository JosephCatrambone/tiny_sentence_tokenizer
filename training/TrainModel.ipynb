{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea61d31-5588-4b3d-a491-906794cc3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "94ea08c6-53d1-4183-9b04-a12ed46c878e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joseph/RustSource/tiny_sentence_tokenizer/training/wandb/run-20240630_220245-sv5r8kdy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/josephc/tiny_sentence_tokenizer/runs/sv5r8kdy' target=\"_blank\">jumping-field-11</a></strong> to <a href='https://wandb.ai/josephc/tiny_sentence_tokenizer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/josephc/tiny_sentence_tokenizer' target=\"_blank\">https://wandb.ai/josephc/tiny_sentence_tokenizer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/josephc/tiny_sentence_tokenizer/runs/sv5r8kdy' target=\"_blank\">https://wandb.ai/josephc/tiny_sentence_tokenizer/runs/sv5r8kdy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/josephc/tiny_sentence_tokenizer/runs/sv5r8kdy?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f895ca41300>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 256\n",
    "HIDDEN_SIZE = 16\n",
    "NUM_EPOCHS = 4\n",
    "CONTEXT_SIZE = 64\n",
    "\n",
    "EXTRA_MODEL_PARAMS = dict(hidden_size=HIDDEN_SIZE, num_heads=4, embedding_size=32, num_outputs=2)\n",
    "\n",
    "experiment_config = {\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": \"transformer\",\n",
    "    \"dataset\": \"synthetic-wiki-one-meelyun-sentences\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"hidden_size\": HIDDEN_SIZE,\n",
    "    \"context_size\": CONTEXT_SIZE,\n",
    "    \"extra_model_params\": EXTRA_MODEL_PARAMS\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    project=\"tiny_sentence_tokenizer\",  # https://wandb.ai/josephc/tiny_sentence_tokenizer\n",
    "    config=experiment_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a543ea6-133e-4403-bb17-924954a8b569",
   "metadata": {},
   "source": [
    "# Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a5d5fc7f-e32e-4c29-b824-99e71d4b9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import bz2\n",
    "import os\n",
    "import random\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentenceSplitDataset:\n",
    "    def __init__(self, path_or_sentences: Union[List, os.PathLike], context_size: int, sentence_breaks: List[str] = [\" \", \"  \", \"\\n\", \"\\r\\n\", \"\\t\", \"\", \"\\n\\n\\n\\n\", \"    \"]):\n",
    "        self.context_size = context_size\n",
    "        self.characters_read = 0\n",
    "        self.sentence_offsets = list()\n",
    "        self.sentences = list()\n",
    "        self.sentence_breaks = sentence_breaks\n",
    "        \n",
    "        input_lines = None\n",
    "        fin = None\n",
    "        if isinstance(path_or_sentences, list):\n",
    "            input_lines = path_or_sentences\n",
    "        else:\n",
    "            fin = bz2.open(path_or_sentences, 'rt')\n",
    "            input_lines = fin\n",
    "        for line in input_lines:\n",
    "            self.sentence_offsets.append(self.characters_read)\n",
    "            line = line.strip()\n",
    "            self.sentences.append(line)\n",
    "            self.characters_read += len(line)+1\n",
    "        if fin is not None:\n",
    "            fin.close()\n",
    "    def __len__(self):\n",
    "        #return len(self.sentences)\n",
    "        return self.characters_read\n",
    "    def get_sentence(self, idx: int) -> str:\n",
    "        return self.sentences[idx]\n",
    "    def get_sentence_idx_with_character(self, idx: int) -> int:\n",
    "        return bisect.bisect_right(self.sentence_offsets, idx)-1\n",
    "    def get_sentence_with_character(self, idx: int, context: Optional[int] = None) -> str:\n",
    "        # Map the index to the sentence.\n",
    "        sentence_idx = self.get_sentence_idx_with_character(idx)\n",
    "        sentence = self.sentences[sentence_idx]\n",
    "        position_in_sentence = idx - self.sentence_offsets[sentence_idx]\n",
    "        assert position_in_sentence >= 0\n",
    "        if context is None:\n",
    "            return sentence\n",
    "        return sentence[max(0, position_in_sentence-context):position_in_sentence], sentence[position_in_sentence:position_in_sentence+context]\n",
    "    def __getitem__(self, idx: int):\n",
    "        prefix, suffix = self.get_sentence_with_character(idx, context=self.context_size)\n",
    "        end_of_sentence = len(suffix) == 0\n",
    "        suffix += random.choice(self.sentence_breaks) + self.get_sentence(self.get_sentence_idx_with_character(idx+1))\n",
    "        prefix = prefix.rjust(self.context_size)[-self.context_size:]\n",
    "        suffix = suffix.ljust(self.context_size)[:self.context_size]\n",
    "        #suffix = suffix.ljust(self.context_size)\n",
    "        return prefix, suffix, end_of_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c22e09c-9465-42b2-a68a-e278ebf7d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SentenceSplitDataset(path_to_sentences=\"./one_meelyun_sentences.bz2\", context_size=CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee2482b3-ce54-4a18-90aa-dd539a0774e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('                        ', 'County and municipal cou', False)\n",
      "('            The Exchange', ' Building opened in 1854', False)\n",
      "('pened in 1854, part of t', 'he building was later us', False)\n",
      "('ened in 1854, part of th', 'e building was later use', False)\n",
      "('ned in 1854, part of the', ' building was later used', False)\n",
      "69: ('lected every four years.', '  The by-census indicate', True)\n",
      "215: (' by more than 1 million.', \" 'On the Marble Cliffs' \", True)\n",
      "378: (\"ion in Hitler's Germany.\", '\\tHomer brief description', True)\n",
      "423: (\"ion in the 'Iliad'Homer.\", '\\n\\n\\n\\nPublic hearings were', True)\n",
      "545: ('co, and Washington, D.C.', '\\tOn July 10, both forces', True)\n",
      "596: ('ced each other in Kyoto.', \"    Monmouth's status as\", True)\n",
      "733: (' November 2002 election.', '\\n\\n\\n\\nOpiates are hypothes', True)\n",
      "796: ('ate aggression and rage.', ' The town celebrated its', True)\n",
      "840: (' its centennial in 2004.', '  In 1681 Anthony Ashley', True)\n",
      "964: (' or recourse to a trial.', \" Crater Lake's features \", True)\n",
      "1202: ('n from July to October).', '    The dam blocked the ', True)\n",
      "1284: ('upstream spawning areas.', '  Antisubmarine measures', True)\n",
      "1396: ('waters since July 1942 .', '\\n\\n\\n\\nOn February 28, 1986', True)\n",
      "1624: ('was left of the station.', '\\nHowever, the coal depos', True)\n",
      "1702: (' amounts of methane gas.', '\\n\\n\\n\\nWhile slightly more ', True)\n",
      "1821: ('gressive foreign policy.', '\\r\\nIn Solomon, the CU wou', True)\n"
     ]
    }
   ],
   "source": [
    "print(ds[0])\n",
    "print(ds[5001])\n",
    "print(ds[5036])\n",
    "print(ds[5037])\n",
    "print(ds[5038])\n",
    "for i in range(0, 2000):\n",
    "    a, b, c = ds[i]\n",
    "    if c:\n",
    "        print(f\"{i}: {ds[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2d011584-12a5-4838-8fdb-7cee8bcce7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import bz2\n",
    "import os\n",
    "import random\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BalancedEOSDataset:\n",
    "    def __init__(\n",
    "            self, \n",
    "            path_or_sentences: Union[List[str], os.PathLike], \n",
    "            context_size: int, \n",
    "            sentence_breaks: List[str] = [\"\", \" \", \" \", \" \", \"  \", \"\\n\", \"\\r\\n\", \"\\t\", \"\\n\\n\\n\\n\", \"    \",],\n",
    "            bad_splits: List[str] = [\"\\n\", \"\\n\", \"\\n\", \"\\n\\n\", \"\\n\\n\\n\", \"\\r\\n\", \"\\n\\t\", \"\\n>\"],\n",
    "            p_bad_split: float = 0.01,\n",
    "            p_change_punctuation: float = 0.1,\n",
    "            p_drop_punctuation: float = 0.001,\n",
    "            p_all_lower: float = 0.1,\n",
    "            p_all_caps: float = 0.1,\n",
    "    ):\n",
    "        self.context_size = context_size\n",
    "        self.sentences = list()\n",
    "        self.sentence_breaks = sentence_breaks\n",
    "        self.bad_splits = bad_splits\n",
    "        self.p_bad_split = p_bad_split\n",
    "        self.p_change_punctuation = p_change_punctuation\n",
    "        self.p_drop_punctuation = p_drop_punctuation\n",
    "        self.p_all_lower = p_all_lower\n",
    "        self.p_all_caps = p_all_caps\n",
    "        if isinstance(path_or_sentences, list):\n",
    "            self.sentences = path_or_sentences\n",
    "        else:\n",
    "            with bz2.open(path_or_sentences, 'rt') as fin:\n",
    "                for line in fin:\n",
    "                    line = line.strip()\n",
    "                    self.sentences.append(line)\n",
    "    def __len__(self):\n",
    "        return (len(self.sentences)-2)*2  # Double since evens will be 'not end of sentence'.\n",
    "    def __getitem__(self, idx: int):\n",
    "        end_of_sentence = (idx%2 != 0)\n",
    "        if not end_of_sentence:\n",
    "            sentence = self.sentences[idx//2]\n",
    "            split_point = random.randint(1, len(sentence)-1)\n",
    "            prefix = sentence[:split_point]\n",
    "            suffix = sentence[split_point:]\n",
    "            prefix = random.choice(self.sentences) + \" \" + prefix\n",
    "            suffix = suffix + \" \" + random.choice(self.sentences)\n",
    "            if random.random() < self.p_bad_split:  # Small chance of a bad split:\n",
    "                prefix += random.choice(self.bad_splits)\n",
    "            prefix = prefix.rjust(self.context_size)\n",
    "            suffix = suffix.ljust(self.context_size)\n",
    "        else: # End of sentence.\n",
    "            prefix = self.sentences[idx//2]\n",
    "            if random.random() > 0.5: # 50/50 shot of just padding vs adding a previous sentence\n",
    "                prefix = random.choice(self.sentences) + \" \" + prefix\n",
    "            if prefix[-1] == \".\" and random.random() < self.p_change_punctuation:\n",
    "                prefix = prefix[:-1] + random.choice([\".\", \"!\", \"?\"])  # Small chance to omit, too.\n",
    "            if prefix[-1] == \".\" and random.random() < self.p_drop_punctuation:\n",
    "                prefix = prefix[:-1]\n",
    "            prefix = prefix.rjust(self.context_size+1)\n",
    "            suffix = self.sentences[(idx//2)+1]\n",
    "            if random.random() > 0.5:\n",
    "                suffix = suffix + \" \" + random.choice(self.sentences)\n",
    "            suffix = (random.choice(self.sentence_breaks) + suffix.ljust(self.context_size))\n",
    "        # Cap size at th same number of _bytes_.\n",
    "        prefix = prefix[-self.context_size:]\n",
    "        suffix = suffix[:self.context_size]\n",
    "        if random.random() < self.p_all_lower:\n",
    "            prefix = prefix.lower()\n",
    "            suffix = suffix.lower()\n",
    "        elif random.random() < self.p_all_caps:\n",
    "            prefix = prefix.upper()\n",
    "            suffix = suffix.upper()\n",
    "        return prefix, suffix, end_of_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "329c5ae2-9285-47d9-b16d-9149a7581681",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BalancedEOSDataset(\"./one_meelyun_sentences.bz2\", context_size=CONTEXT_SIZE)\n",
    "#ds = BalancedEOSDataset(ds.sentences, context_size=CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "775efbc9-983a-403a-99f6-2cfff1a7df3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('redit for the torigking expedition. county and municipal council', 's are popularly elected every four years. the first two years of', False)\n",
      "('sure of President George W. Bush and Vice President Dick Cheney.', ' Sabine Baring-Gould says of this saint \"he was much edified wit', True)\n",
      "(' was routine administration and quite limited. By 2006, there we', 're 36 airports and one heliport. The final stage consisted of a ', False)\n",
      "('               BY 2006, THERE WERE 36 AIRPORTS AND ONE HELIPORT.', '    KENNEDY LATER SAID THAT HIS FOUR DAY-VISIT TO IRELAND WAS ON', True)\n",
      "('that his four day-visit to Ireland was one of his most enjoyable', '. Nevertheless, because non-Mormons in the east would not have c', False)\n"
     ]
    }
   ],
   "source": [
    "print(ds[0])\n",
    "print(ds[5001])\n",
    "print(ds[5036])\n",
    "print(ds[5037])\n",
    "print(ds[5038])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b1e7e0dc-db8a-435b-944e-8a555181e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sentences = [\n",
    "    \"Hello, world!\",\n",
    "    \"Dr. M. Emdee, M.D. said that her upbringing in Washington D.C. was formative.\",\n",
    "    \"He said, \\\"Hey, can you hear me?\\\"\",\n",
    "    \"Obviously, this is self-contained.\",\n",
    "    \"I think it's safe to say that this is self contained: a container.\",\n",
    "    \"A statement followed by a question and separated by a colon: an effective journalistic technique?\",\n",
    "    \"Every room is a panic room if you just give me a minute.\",\n",
    "    \"Has anyone really been as far as want even to look more like?\",\n",
    "    \"The... the thing?\",\n",
    "    \"1. The first number.\",\n",
    "    '\"Bruh, can you believe it?\"',\n",
    "    '\"I can\\'t believe it bruh.\"',\n",
    "]\n",
    "# NOTE: Using the SentenceSplitDataset instead of the BalancedEOSDataset.\n",
    "validate_ds = SentenceSplitDataset(path_or_sentences=validation_sentences, context_size=CONTEXT_SIZE, sentence_breaks=[\" \", \"\\n\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0e866-ed81-40bb-8b58-9a75fcaced6d",
   "metadata": {},
   "source": [
    "# Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0901334-6714-46dc-bb56-3a7b4517421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Union\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(256, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def strings_to_tensor(sentences: list) -> torch.Tensor:\n",
    "        longest = max([len(sentence.encode(\"utf-8\")) for sentence in sentences])\n",
    "        out = torch.zeros(len(sentences), longest, 256)\n",
    "        for batch_idx, s in enumerate(sentences):\n",
    "            s = s.encode(\"utf-8\")\n",
    "            for byte_idx in range(len(s)):\n",
    "                out[batch_idx, byte_idx, int(s[byte_idx])] = 1.0\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, hidden = None):\n",
    "        hidden = F.tanh(self.i2h(x) + self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def _init_hidden(self, height: int = 1):\n",
    "        return torch.zeros(height, self.hidden_size)\n",
    "\n",
    "    def split_paragraph_iter(self, p: str, min_threshold: Optional[float] = None) -> Iterator[str]:\n",
    "        self.eval()\n",
    "        h = self._init_hidden()\n",
    "        i = torch.zeros(1, 256)\n",
    "        last_sentence = \"\"\n",
    "        for character in p:\n",
    "            last_sentence += character\n",
    "            # Convert character to a byte.\n",
    "            b = character.encode(\"utf-8\")\n",
    "            for b_value in b:\n",
    "                i[0, int(b_value)] = 1.0\n",
    "                out, h = self.forward(i, h)\n",
    "                i[0, int(b_value)] = 0.0\n",
    "            out = out.cpu().numpy()\n",
    "            if out[0,1] >= out[0,0] or (min_threshold is not None and out[0,1] > min_threshold):\n",
    "                yield last_sentence\n",
    "                last_sentence = \"\"\n",
    "        yield last_sentence\n",
    "\n",
    "\n",
    "def run_inference(m, prefix: List[str], suffix: Optional[List[str]]):\n",
    "    prefix = RNN.strings_to_tensor(prefix).to(DEVICE)\n",
    "    if suffix is not None:\n",
    "        suffix = RNN.strings_to_tensor(suffix).to(DEVICE)\n",
    "    hidden = m._init_hidden().to(DEVICE)\n",
    "    for i in range(0, prefix.shape[1]):\n",
    "        out, hidden = m(prefix[:,i,:], hidden)\n",
    "    if suffix is not None:\n",
    "        for i in range(0, suffix.shape[1]):\n",
    "            out, hidden = m(suffix[:,i,:], hidden)\n",
    "    return out\n",
    "\n",
    "\n",
    "n_hidden = HIDDEN_SIZE\n",
    "n_categories = 2\n",
    "model = RNN(n_hidden, n_categories)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2707b139-7937-4924-86f5-f4acb24b0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Union\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from unidecode import unidecode\n",
    "\n",
    "class TFSentenceSplit(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, embedding_size: int, num_outputs: int):\n",
    "        super().__init__()\n",
    "        self.position_embedding = nn.Embedding(num_embeddings=256, embedding_dim=embedding_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_size, nhead=num_heads, batch_first=True)\n",
    "        self.inference_head = nn.Linear(embedding_size, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assumes x is batch first like a normal, sensible person.\n",
    "        # out = self.encode_layer(torch.rand(batch_size, seq_length, embedding_size))\n",
    "        # Use torch.LongTensor to encode.\n",
    "        x = self.position_embedding(x)\n",
    "        x = self.encoder_layer(x) # Out: [batch_size, seq_len, embedding_size]\n",
    "        x = self.inference_head(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x[:,-1,:].squeeze(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def strings_to_tensor(sentences: list) -> torch.Tensor:\n",
    "        longest = max([len(sentence.encode(\"utf-8\")) for sentence in sentences])\n",
    "        out = torch.zeros((len(sentences), longest), dtype=torch.int64)  # torch.LongTensor\n",
    "        for batch_idx, s in enumerate(sentences):\n",
    "            s = s.rjust(longest).encode(\"utf-8\")[-longest:]  # Pad the left with spaces so it's aligned, then convert to bytes and truncate.\n",
    "            for byte_idx in range(len(s)):\n",
    "                out[batch_idx, byte_idx] = int(s[byte_idx])\n",
    "        return out\n",
    "\n",
    "def run_inference(m, prefix: List[str], suffix: List[str]):\n",
    "    prefix = TFSentenceSplit.strings_to_tensor(prefix).to(DEVICE)\n",
    "    #suffix = TFSentenceSplit.strings_to_tensor(suffix).to(DEVICE)\n",
    "    return m.forward(prefix)\n",
    "\n",
    "model = TFSentenceSplit(**EXTRA_MODEL_PARAMS)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5dd52701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Union\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from unidecode import unidecode\n",
    "\n",
    "class BidirectionalTransformerSentenceSplit(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, embedding_size: int, num_outputs: int):\n",
    "        super().__init__()\n",
    "        self.position_embedding = nn.Embedding(num_embeddings=256, embedding_dim=embedding_size)\n",
    "        self.prefix_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_size, nhead=num_heads, batch_first=True)\n",
    "        self.suffix_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_size, nhead=num_heads, batch_first=True)\n",
    "        self.inference_head = nn.Linear(embedding_size, num_outputs)\n",
    "\n",
    "    def forward(self, prefix, suffix):\n",
    "        # Assumes x is batch first like a normal, sensible person.\n",
    "        # out = self.encode_layer(torch.rand(batch_size, seq_length, embedding_size))\n",
    "        # Use torch.LongTensor to encode.\n",
    "        prefix = self.position_embedding(prefix)\n",
    "        suffix = self.position_embedding(suffix)\n",
    "        x = self.prefix_encoder_layer(prefix) + self.suffix_encoder_layer(suffix) # Out: [batch_size, seq_len, embedding_size]\n",
    "        x = self.inference_head(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x[:,-1,:].squeeze(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def strings_to_tensor(sentences: list) -> torch.Tensor:\n",
    "        longest = max([len(sentence.encode(\"utf-8\")) for sentence in sentences])\n",
    "        out = torch.zeros((len(sentences), longest), dtype=torch.int64)  # torch.LongTensor\n",
    "        for batch_idx, s in enumerate(sentences):\n",
    "            s = s.rjust(longest).encode(\"utf-8\")[-longest:]  # Pad the left with spaces so it's aligned, then convert to bytes and truncate.\n",
    "            for byte_idx in range(len(s)):\n",
    "                out[batch_idx, byte_idx] = int(s[byte_idx])\n",
    "        return out\n",
    "\n",
    "def run_inference(m, prefix: List[str], suffix: List[str]):\n",
    "    prefix = TFSentenceSplit.strings_to_tensor(prefix).to(DEVICE)\n",
    "    suffix = TFSentenceSplit.strings_to_tensor(suffix).to(DEVICE)\n",
    "    width = min(prefix.shape[1], suffix.shape[1])\n",
    "    return m.forward(prefix[:,-width:], suffix[:,:width])\n",
    "\n",
    "model = BidirectionalTransformerSentenceSplit(**EXTRA_MODEL_PARAMS)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e1a77-d8e3-49c8-9484-e35857d655bd",
   "metadata": {},
   "source": [
    "# Evaluation and Training Prep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8e384879-1a7d-4d06-b6ca-65b646838ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "total_examples = len(ds)  # Hack -- dunno the total example count.\n",
    "train_size = int(total_examples*0.9)\n",
    "validation_size = int(total_examples - train_size)\n",
    "#test_size = total_examples - (train_size + validation_size)\n",
    "#train_ds, validate_ds, test_ds = torch.utils.data.random_split(ds, [train_size, validation_size, test_size])\n",
    "#train_ds, validate_ds = torch.utils.data.random_split(ds, [train_size, validation_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3bd259da-7f93-4d9d-97fd-fdaf8dc5ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#validate_dataloader = DataLoader(validate_ds, batch_size=BATCH_SIZE)\n",
    "validate_dataloader = DataLoader(validate_ds, batch_size=BATCH_SIZE)  # Note this is defined in the data at the top.\n",
    "#test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5f9157e1-d65d-4def-bc1c-678e7cbc5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tpr_fpr_tnr_fnr(predictions_logits, ground_truth_labels):\n",
    "    # Assume predictions are a tensor of shape [batch, 2], same with ground truth.\n",
    "    # Assume predictions are NORMALIZED along axis=1 ([_,0], [_,1]).\n",
    "    # Assume [:,1] means 'yes, this is a break'.\n",
    "    with torch.no_grad():\n",
    "        pred = predictions_logits.cpu().numpy()\n",
    "        gt = ground_truth_labels.cpu().numpy()\n",
    "        \n",
    "        tpr = 0\n",
    "        fpr = 0\n",
    "        tnr = 0\n",
    "        fnr = 0\n",
    "        for idx in range(0, pred.shape[0]):\n",
    "            if gt[idx] < 0.5 or gt[idx] == False: # GT: Negative\n",
    "                if pred[idx,0] > pred[idx,1]: # Pred: Negative\n",
    "                    tnr += 1\n",
    "                else: # Pred: Positive\n",
    "                    fpr += 1\n",
    "            else: # GT: Positive\n",
    "                if pred[idx,0] > pred[idx,1]: # Pred: Negative:\n",
    "                    fnr += 1\n",
    "                else: # Pred: Positive\n",
    "                    tpr += 1\n",
    "        return tpr, fpr, tnr, fnr\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8d53297c-495e-43a2-ae18-ba9fa60542ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 4, 1, 0)\n",
      "(4, 0, 0, 1)\n",
      "tensor([[0.6693, 0.3307]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "(0, 0, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "assert compute_tpr_fpr_tnr_fnr(torch.tensor([[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]), torch.tensor([0, 1, 0, 1])) == (1, 1, 1, 1)\n",
    "print(compute_tpr_fpr_tnr_fnr(torch.tensor([[0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.6, 0.4]]), torch.tensor([0, 0, 0, 0, 0])))\n",
    "print(compute_tpr_fpr_tnr_fnr(torch.tensor([[0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.6, 0.4]]), torch.tensor([1, 1, 1, 1, 1])))\n",
    "print(run_inference(model, [\"Only a.\"], [\"butthead\"]))\n",
    "print(compute_tpr_fpr_tnr_fnr(run_inference(model, [\"Only a test\", \"Mostly a test\", \"Ignore me\"], [\"aaaa\", \"bbbb\", \"CCCC\"]), torch.tensor([0, 1, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "862cf75d-100b-4901-956f-3062b9950fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 79, 110, 108, 121,  32,  97,  32, 116, 101, 115, 116,  46],\n",
      "        [ 32,  32,  32,  32,  77, 111, 115, 116, 108, 121,  32,  97],\n",
      "        [ 32,  32,  32,  32,  32,  32,  73, 103, 110, 111, 114, 101]])\n",
      "tensor([[0.6615, 0.3385]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "(0, 0, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "print(TFSentenceSplit.strings_to_tensor([\"Only a test.\", \"Mostly a\", \"Ignore\"]))\n",
    "print(run_inference(model, [\"Only a test.\"], [\"Test\"]))\n",
    "print(compute_tpr_fpr_tnr_fnr(run_inference(model, [\"Only a test.\", \"Mostly a\", \"Ignore\"], [\"aaaa\", \"bbbb\", \"CCCC\"]), torch.tensor([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf0e588-18e8-478a-a5a4-4b35f8df0746",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "01d9b145-37f7-4674-99ae-003bc60f044a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58d45c538db4860969fea6d377ec052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e83b4a222544efe98cfed4d22242b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 999: 0.0013630061632433597\n",
      "0: 1999: 0.0013599603097932465\n",
      "0: 2999: 0.0013660601872827424\n",
      "0: 3999: 0.0013610136676452153\n",
      "0: 4999: 0.0013682807486761607\n",
      "0: 5999: 0.0013526185856018698\n",
      "0: 6999: 0.0013522559090553704\n",
      "END OF EPOCH 0: 10.617626822553575 train loss\n",
      "END OF EPOCH 0: 0.004185141297057271 validation loss\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842c59e974064171bc75475177662556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 999: 0.0013691968594523043\n",
      "1: 1999: 0.001355107447403433\n",
      "1: 2999: 0.0013617549777748957\n",
      "1: 3999: 0.0013590342843573405\n",
      "1: 4999: 0.0013720859770224557\n",
      "1: 5999: 0.0013543043850086007\n",
      "1: 6999: 0.001370117235383661\n",
      "END OF EPOCH 1: 10.607014010311104 train loss\n",
      "END OF EPOCH 1: 0.004185141180641949 validation loss\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd31e8f92354e208e28e73a2eced43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 999: 0.0013566519491371263\n",
      "2: 1999: 0.0013708272551828863\n",
      "2: 2999: 0.0013403854504217352\n",
      "2: 3999: 0.0013586299205739414\n",
      "2: 4999: 0.0013731517354719314\n",
      "2: 5999: 0.0013493571773274234\n",
      "2: 6999: 0.001373983475450804\n",
      "END OF EPOCH 2: 10.60801824531518 train loss\n",
      "END OF EPOCH 2: 0.004185141529887915 validation loss\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519332a7a3fc4de7ba098915d2ef2d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: 999: 0.00135787644987126\n",
      "3: 1999: 0.0013710970381047237\n",
      "3: 2999: 0.0013482916651648977\n",
      "3: 3999: 0.0013588632386499326\n",
      "3: 4999: 0.0013534841039215346\n",
      "3: 5999: 0.0013462657834074948\n",
      "3: 6999: 0.0013655029102604248\n",
      "END OF EPOCH 3: 10.608599066152237 train loss\n",
      "END OF EPOCH 3: 0.004185141529887915 validation loss\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁███████████████████████████████████</td></tr><tr><td>batch_loss</td><td>█▂▁▂▁▁▁▁▁▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▂▁▁▂</td></tr><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>f1</td><td>▁███████████████████████████████████</td></tr><tr><td>false_negative_rate</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>false_positive_rate</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▁███████████████████████████████████</td></tr><tr><td>recall</td><td>▁███████████████████████████████████</td></tr><tr><td>true_negative_rate</td><td>▁████████▇█████████████████▅████████</td></tr><tr><td>true_positive_rate</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>▁▁██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.96367</td></tr><tr><td>batch_loss</td><td>0.00141</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>f1</td><td>0.96235</td></tr><tr><td>false_negative_rate</td><td>0.03461</td></tr><tr><td>false_positive_rate</td><td>0.00172</td></tr><tr><td>precision</td><td>0.99631</td></tr><tr><td>recall</td><td>0.93062</td></tr><tr><td>true_negative_rate</td><td>0.49945</td></tr><tr><td>true_positive_rate</td><td>0.46422</td></tr><tr><td>validation_loss</td><td>0.00419</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jumping-field-11</strong> at: <a href='https://wandb.ai/josephc/tiny_sentence_tokenizer/runs/sv5r8kdy' target=\"_blank\">https://wandb.ai/josephc/tiny_sentence_tokenizer/runs/sv5r8kdy</a><br/> View project at: <a href='https://wandb.ai/josephc/tiny_sentence_tokenizer' target=\"_blank\">https://wandb.ai/josephc/tiny_sentence_tokenizer</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240630_220245-sv5r8kdy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "lowest_loss = 1e10\n",
    "best_counts = 0\n",
    "\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    examples_seen = 0\n",
    "    positives_seen = 0\n",
    "    total_train_loss = 0.0\n",
    "    running_train_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    tpr, tnr, fpr, fnr = 0, 0, 0, 0\n",
    "    for batch_idx, (pre, suf, label) in tqdm(enumerate(train_dataloader)):\n",
    "        label = (torch.Tensor(label) * 1).to(DEVICE)\n",
    "        out = run_inference(model, pre, suf)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        examples_seen += BATCH_SIZE\n",
    "        positives_seen += torch.sum(label).item()\n",
    "        per_example_loss = loss.item()/BATCH_SIZE\n",
    "        total_train_loss += per_example_loss\n",
    "        running_train_loss = running_train_loss * 0.9 + per_example_loss * 0.1\n",
    "        batch_tp, batch_fp, batch_tn, batch_fn = compute_tpr_fpr_tnr_fnr(out, label)\n",
    "        tpr += batch_tp\n",
    "        tnr += batch_tn\n",
    "        fpr += batch_fp\n",
    "        fnr += batch_fn\n",
    "        if batch_idx % 100 == 0:\n",
    "            rate = tpr+tnr+fpr+fnr\n",
    "            precision = tpr/float(tpr+fpr)\n",
    "            recall = tpr/float(tpr+fnr)\n",
    "            f1 = (2*precision*recall)/(precision + recall)\n",
    "            wandb.log({\n",
    "            #noop = dict(foo={\n",
    "                \"batch_loss\": per_example_loss, \n",
    "                \"false_positive_rate\": fpr/rate, \n",
    "                \"true_positive_rate\": tpr/rate,\n",
    "                \"false_negative_rate\": fnr/rate,\n",
    "                \"true_negative_rate\": tnr/rate,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"accuracy\": (tpr+tnr)/rate\n",
    "            }, commit=(batch_idx%1000)==0)\n",
    "            tpr, tnr, fpr, fnr = 0, 0, 0, 0\n",
    "        if (batch_idx+1) % 1000 == 0:\n",
    "            print(f\"{epoch}: {batch_idx}: {running_train_loss}\")\n",
    "    print(f\"END OF EPOCH {epoch}: {total_train_loss} train loss\")\n",
    "    model.eval()\n",
    "    for batch_idx, (pre, suf, label) in enumerate(validate_dataloader):\n",
    "        label = (torch.Tensor(label) * 1).to(DEVICE)\n",
    "        out = run_inference(model, pre, suf)\n",
    "        loss = loss_fn(out, label)\n",
    "        validation_loss += loss.item()/BATCH_SIZE\n",
    "    wandb.log({\"epoch\": epoch, \"validation_loss\": validation_loss})\n",
    "    print(f\"END OF EPOCH {epoch}: {validation_loss} validation loss\")\n",
    "    torch.save(model, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "    if validation_loss < lowest_loss:\n",
    "        lowest_loss = validation_loss\n",
    "        torch.save(model, f\"best_{best_counts}.pt\")\n",
    "        best_counts += 1\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9cd0f5-3410-4d62-86e8-3c349fdde81b",
   "metadata": {},
   "source": [
    "Lifted from https://damdid2022.frccsc.ru/files/article/DAMDID_2022_paper_2646.pdf\n",
    "Zavyalova, Martynyuk, and Samarev\n",
    "\n",
    "\"Testing will be performed on 5840 sentences from “The GUM Corpus” [16].\"\n",
    "\n",
    "|Rank|Tool Name                |tp  |fp | tn   |fn  |accuracy|error|precision|recall|f1   |\n",
    "|---|---                      |--- |---|---   |--- |---     |---  |---      |---   |---  |\n",
    "|1  |Sentencize.jl            |6330|254|107813|1078|0.99    |0.01 |0.96     |0.85  |0.905|\n",
    "|2  |NLTK                     |6269|283|107787|1139|0.99    |0.01 |0.96     |0.85  |0.898|\n",
    "|3  |OpenNLP                  |6255|276|107791|1153|0.99    |0.01 |0.96     |0.84  |0.897|\n",
    "|4  |CoreNLP                  |6278|362|107786|1130|0.99    |0.01 |0.95     |0.85  |0.894|\n",
    "|5  |WordTokenizers.jl        |6140|264|107809|1268|0.99    |0.01 |0.96     |0.83  |0.889|\n",
    "|6  |Spacy (Dependency parser)|6631|934|107268|777 |0.99    |0.01 |0.88     |0.90  |0.886|\n",
    "|7  |Spacy (Rule-based)       |6183|994|107531|1225|0.98    |0.02 |0.86     |0.83  |0.848|\n",
    "|8  |SimpleSplitter           |5760|772|107847|1648|0.98    |0.02 |0.88     |0.78  |0.826|\n",
    "|9  |Julia split()            |5760|878|107780|1648|0.98    |0.02 |0.87     |0.78  |0.820|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "503471d0-b86f-4676-80e1-4f062f363f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:\n",
      "('fighting class between that of a squire and a page). County and ', 'municipal councils are popularly elected every four years. While', False)\n",
      "DL raw:\n",
      "D UNTIL 1433, BUT REFUSED TO BUY HIS RELEASE BY ABANDONING HIS C\n",
      "tensor(False)\n",
      "Processed:\n",
      "TP: 128  TN: 118  FP: 0  FN: 10\n",
      "!!!\n",
      "Sent: s, and some scholars interpret this as a form of contraception.\"\n",
      "Model guess: EOS: False\n",
      "Truth: EOS: True\n",
      "\n",
      "!!!\n",
      "Sent: of chicory by farm animals results in reduction of worm burdens,\n",
      "Model guess: EOS: False\n",
      "Truth: EOS: True\n",
      "\n",
      "!!!\n",
      "Sent: founded on the ruins of besa where he died (dio cassius 'lix.11;\n",
      "Model guess: EOS: False\n",
      "Truth: EOS: True\n",
      "\n",
      "!!!\n",
      "Sent: ph journal, sackville/edmundston, nb, canada, newsroom@nbpub.com\n",
      "Model guess: EOS: False\n",
      "Truth: EOS: True\n",
      "\n",
      "!!!\n",
      "Sent: form a network of nodes &mdash; hence the term \"neural network.\"\n",
      "Model guess: EOS: False\n",
      "Truth: EOS: True\n",
      "\n",
      "!!!\n",
      "Sent: to become a better christian by learning from his mistakes.pugh,\n",
      "Model guess: EOS: False\n",
      "Truth: EOS: True\n",
      "\n",
      "!!!\n",
      "Sent: tin Tarantino\" could be used in advertising and onscreen.Biskind\n",
      "Model guess: EOS: False\n",
      "Truth: EOS: True\n",
      "\n",
      "!!!\n",
      "Sent: ong of the time called \"The Bug-Out Ballad\". Green Heron U U U U\n",
      "Model guess: EOS: False\n",
      "Truth: EOS: True\n",
      "\n",
      "!!!\n",
      "Sent: d bridge in Hannibal, Missouri, and a bridge in Quincy, Illinois\n",
      "Model guess: EOS: False\n",
      "Truth: EOS: True\n",
      "\n",
      "!!!\n",
      "Sent: esulting in long-standing disputes over the frontier zones.lynn:\n",
      "Model guess: EOS: False\n",
      "Truth: EOS: True\n",
      "\n",
      "Total errors: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(\"Raw:\")\n",
    "print(ds[0])\n",
    "print(\"DL raw:\")\n",
    "for pre, suf, label in train_dataloader:\n",
    "    print(pre[0])\n",
    "    print(label[0])\n",
    "    break\n",
    "print(\"Processed:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for pre, suf, label in train_dataloader:\n",
    "        label = (torch.Tensor(label) * 1).to(DEVICE)\n",
    "        out = run_inference(model, pre, suf)\n",
    "        tp, fp, tn, fn = compute_tpr_fpr_tnr_fnr(out, label)\n",
    "        loss = loss_fn(out, label).cpu().numpy()\n",
    "        out = out.cpu().numpy()\n",
    "        confidence = numpy.abs(out[:, 0] - out[:, 1])\n",
    "        print(f\"TP: {tp}  TN: {tn}  FP: {fp}  FN: {fn}\")\n",
    "        errors = 0\n",
    "        for idx in range(out.shape[0]):\n",
    "            model_guess_eos = out[idx,1]>out[idx,0]\n",
    "            gt_eos = label[idx]>0.5\n",
    "            if model_guess_eos != gt_eos:\n",
    "                print(\"!!!\")\n",
    "                errors += 1\n",
    "                print(f\"Sent: {pre[idx]}\")\n",
    "                print(f\"Model guess: EOS: {model_guess_eos}\")\n",
    "                print(f\"Truth: EOS: {gt_eos}\")\n",
    "                print()\n",
    "        print(f\"Total errors: {errors}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "cb15e897-2748-4bb3-b3fe-9b8712aaeeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/symbolic_opset11.py:958: UserWarning: This model contains a squeeze operation on dimension 1. The size of this dimension in the given input is 2. The model will be exported without the squeeze node. If the model is intended to be used with dynamic input shapes, please export with dynamic_axes argument.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save model:\n",
    "# Prefix only:\n",
    "placeholder_x = torch.zeros([BATCH_SIZE, CONTEXT_SIZE], dtype=torch.int64)\n",
    "model.eval().to('cpu')\n",
    "out = model(placeholder_x)\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    placeholder_x,\n",
    "    f\"sentence_tokenizer_v11_{CONTEXT_SIZE}_256.onnx\",\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    "    opset_version=14,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['input',],   # the model's input names\n",
    "    output_names = ['output'], # the model's output names\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}}\n",
    ")\n",
    "\n",
    "\"\"\" # Prefix + Suffix:\n",
    "placeholder_a = torch.zeros([BATCH_SIZE, CONTEXT_SIZE], dtype=torch.int64)\n",
    "placeholder_b = torch.zeros([BATCH_SIZE, CONTEXT_SIZE], dtype=torch.int64)\n",
    "model.eval().to('cpu')\n",
    "out = model(placeholder_a, placeholder_b)\n",
    "print(out.shape)\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    (placeholder_a, placeholder_b),\n",
    "    f\"sentence_tokenizer_v11_{CONTEXT_SIZE}_prefix_{CONTEXT_SIZE}_suffix_256.onnx\",\n",
    "    export_params=True,        # store the trained parameter weights inside the model file\n",
    "    opset_version=14,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['prefix', 'suffix'],   # the model's input names\n",
    "    output_names = ['output'], # the model's output names\n",
    "    dynamic_axes={'prefix' : {0 : 'batch_size'}, 'suffix' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}}\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ee230b4f-dc30-4a80-bded-8466a8919f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OnnxExporterError",
     "evalue": "Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:1195\u001b[0m, in \u001b[0;36mdynamo_export\u001b[0;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_export_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:961\u001b[0m, in \u001b[0;36mExporter.export\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    958\u001b[0m fx_interpreter \u001b[38;5;241m=\u001b[39m fx_onnx_interpreter\u001b[38;5;241m.\u001b[39mFxOnnxInterpreter(\n\u001b[1;32m    959\u001b[0m     diagnostic_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdiagnostic_context\n\u001b[1;32m    960\u001b[0m )\n\u001b[0;32m--> 961\u001b[0m onnxscript_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfx_graph_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnxfunction_dispatcher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnxfunction_dispatcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_level_debug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop_level_debug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;66;03m# NOTE: Filter out the initializers with fake tensors when it's fake_mode exporting.\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;66;03m# Otherwise, the ONNX exporter will fail: RuntimeError: basic_string::_M_construct null\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# not valid.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# Concrete data is expected to be filled for those initializers later during `ExportOutput.save`.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:151\u001b[0m, in \u001b[0;36mdiagnose_call.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_and_raise_if_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiag\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/context.py:366\u001b[0m, in \u001b[0;36mDiagnosticContext.log_and_raise_if_error\u001b[0;34m(self, diagnostic)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m diagnostic\u001b[38;5;241m.\u001b[39msource_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m diagnostic\u001b[38;5;241m.\u001b[39msource_exception\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m RuntimeErrorWithDiagnostic(diagnostic)\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:135\u001b[0m, in \u001b[0;36mdiagnose_call.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     return_values \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m diag\u001b[38;5;241m.\u001b[39mlog_section(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn values\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:534\u001b[0m, in \u001b[0;36mFxOnnxInterpreter.run\u001b[0;34m(self, fx_graph_module, onnxfunction_dispatcher, op_level_debug, parent_onnxscript_graph)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m fx_graph_module\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[0;32m--> 534\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfx_graph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[43monnxfunction_dispatcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m            \u001b[49m\u001b[43mop_level_debug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m            \u001b[49m\u001b[43monnxscript_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43monnxscript_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfx_name_to_onnxscript_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m diagnostic\u001b[38;5;241m.\u001b[39mlog_section(logging\u001b[38;5;241m.\u001b[39mDEBUG, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNX Graph:\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:151\u001b[0m, in \u001b[0;36mdiagnose_call.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_and_raise_if_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiag\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/context.py:366\u001b[0m, in \u001b[0;36mDiagnosticContext.log_and_raise_if_error\u001b[0;34m(self, diagnostic)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m diagnostic\u001b[38;5;241m.\u001b[39msource_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m diagnostic\u001b[38;5;241m.\u001b[39msource_exception\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m RuntimeErrorWithDiagnostic(diagnostic)\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:135\u001b[0m, in \u001b[0;36mdiagnose_call.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     return_values \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m diag\u001b[38;5;241m.\u001b[39mlog_section(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn values\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:439\u001b[0m, in \u001b[0;36mFxOnnxInterpreter.run_node\u001b[0;34m(self, node, fx_graph_module, onnxfunction_dispatcher, op_level_debug, onnxscript_graph, onnxscript_tracer, fx_name_to_onnxscript_value)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43monnxscript_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfx_name_to_onnxscript_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43monnxscript_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfx_graph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43monnxfunction_dispatcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_level_debug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:763\u001b[0m, in \u001b[0;36mFxOnnxInterpreter.call_module\u001b[0;34m(self, node, parent_onnxscript_graph, fx_name_to_onnxscript_value, tracer, root_fx_graph_module, onnxfunction_dispatcher, op_level_debug)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    760\u001b[0m     sub_module, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule\n\u001b[1;32m    761\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_module must be a torch.fx.GraphModule, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(sub_module)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for node \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 763\u001b[0m sub_onnxscript_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnxfunction_dispatcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_level_debug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_onnxscript_graph\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m onnx_args, _ \u001b[38;5;241m=\u001b[39m _wrap_fx_args_as_onnxscript_args(\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28mlist\u001b[39m(node\u001b[38;5;241m.\u001b[39margs), {}, fx_name_to_onnxscript_value, tracer\n\u001b[1;32m    769\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:151\u001b[0m, in \u001b[0;36mdiagnose_call.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_and_raise_if_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiag\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/context.py:366\u001b[0m, in \u001b[0;36mDiagnosticContext.log_and_raise_if_error\u001b[0;34m(self, diagnostic)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m diagnostic\u001b[38;5;241m.\u001b[39msource_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m diagnostic\u001b[38;5;241m.\u001b[39msource_exception\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m RuntimeErrorWithDiagnostic(diagnostic)\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:135\u001b[0m, in \u001b[0;36mdiagnose_call.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     return_values \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m diag\u001b[38;5;241m.\u001b[39mlog_section(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn values\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:534\u001b[0m, in \u001b[0;36mFxOnnxInterpreter.run\u001b[0;34m(self, fx_graph_module, onnxfunction_dispatcher, op_level_debug, parent_onnxscript_graph)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m fx_graph_module\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[0;32m--> 534\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfx_graph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[43monnxfunction_dispatcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m            \u001b[49m\u001b[43mop_level_debug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m            \u001b[49m\u001b[43monnxscript_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43monnxscript_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfx_name_to_onnxscript_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m diagnostic\u001b[38;5;241m.\u001b[39mlog_section(logging\u001b[38;5;241m.\u001b[39mDEBUG, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNX Graph:\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:151\u001b[0m, in \u001b[0;36mdiagnose_call.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_and_raise_if_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiag\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/context.py:366\u001b[0m, in \u001b[0;36mDiagnosticContext.log_and_raise_if_error\u001b[0;34m(self, diagnostic)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m diagnostic\u001b[38;5;241m.\u001b[39msource_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m diagnostic\u001b[38;5;241m.\u001b[39msource_exception\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m RuntimeErrorWithDiagnostic(diagnostic)\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:135\u001b[0m, in \u001b[0;36mdiagnose_call.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     return_values \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m diag\u001b[38;5;241m.\u001b[39mlog_section(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn values\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:428\u001b[0m, in \u001b[0;36mFxOnnxInterpreter.run_node\u001b[0;34m(self, node, fx_graph_module, onnxfunction_dispatcher, op_level_debug, onnxscript_graph, onnxscript_tracer, fx_name_to_onnxscript_value)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43monnxscript_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfx_name_to_onnxscript_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43monnxfunction_dispatcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_level_debug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfx_graph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_method\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:660\u001b[0m, in \u001b[0;36mFxOnnxInterpreter.call_function\u001b[0;34m(self, node, onnxscript_tracer, fx_name_to_onnxscript_value, onnxfunction_dispatcher, op_level_debug, fx_graph_module)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# Assign type and shape from fx graph.\u001b[39;00m\n\u001b[0;32m--> 660\u001b[0m \u001b[43m_fill_tensor_shape_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# One fx node could produce multiple outputs (e.g., tuple of tensors); in\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# that case, v is a tuple of TorchScriptTensors.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:244\u001b[0m, in \u001b[0;36m_fill_tensor_shape_type\u001b[0;34m(onnxscript_values, name, expected_values)\u001b[0m\n\u001b[1;32m    241\u001b[0m     onnxscript_value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m fx_type_utils\u001b[38;5;241m.\u001b[39mfrom_sym_value_to_torch_dtype(\n\u001b[1;32m    242\u001b[0m         expected_value\n\u001b[1;32m    243\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fx_type_utils\u001b[38;5;241m.\u001b[39mis_torch_complex_dtype(\u001b[43mexpected_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m):\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# Like torch.view_as_real, we flatten complex tensors to real tensors with\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# additional last dimension of 2\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     onnxscript_value\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    249\u001b[0m             dim \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    253\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dtype'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m placeholder_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m1\u001b[39m, CONTEXT_SIZE], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m exporter \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamo_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplaceholder_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m exporter\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_tokenizer_v6_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONTEXT_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx256_dynamo.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/transformers/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:1206\u001b[0m, in \u001b[0;36mdynamo_export\u001b[0;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m resolved_export_options\u001b[38;5;241m.\u001b[39mdiagnostic_context\u001b[38;5;241m.\u001b[39mdump(sarif_report_path)\n\u001b[1;32m   1199\u001b[0m message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to export the model to ONNX. Generating SARIF report at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msarif_report_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSARIF is a standard format for the output of static analysis tools. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease report a bug on PyTorch Github: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_PYTORCH_GITHUB_ISSUES_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1205\u001b[0m )\n\u001b[0;32m-> 1206\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OnnxExporterError(\n\u001b[1;32m   1207\u001b[0m     ExportOutput\u001b[38;5;241m.\u001b[39m_from_failure(e, resolved_export_options\u001b[38;5;241m.\u001b[39mdiagnostic_context),\n\u001b[1;32m   1208\u001b[0m     message,\n\u001b[1;32m   1209\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOnnxExporterError\u001b[0m: Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues"
     ]
    }
   ],
   "source": [
    "placeholder_x = torch.zeros([1, CONTEXT_SIZE], dtype=torch.int64)\n",
    "model.eval().to('cpu')\n",
    "exporter = torch.onnx.dynamo_export(model, placeholder_x)\n",
    "exporter.save(f\"sentence_tokenizer_v6_{CONTEXT_SIZE}x256_dynamo.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
